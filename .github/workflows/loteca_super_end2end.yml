name: Loteca Super End2End

on:
  workflow_dispatch:
  schedule:
    - cron: "15 12 * * *"

jobs:
  super:
    runs-on: ubuntu-latest

    env:
      SEASON: "2025"
      LOOKAHEAD_DAYS: "3"
      REGIONS: "uk,eu,us,au"
      BANKROLL: "1000"
      KELLY_FRACTION: "0.5"
      KELLY_CAP: "0.10"
      KELLY_TOP_N: "14"
      ROUND_TO: "1"
      TRAIN_CALIBRATOR: "true"
      SOURCE_CSV: "data/in/matches_source.csv"
      THEODDS_API_KEY: ${{ secrets.THEODDS_API_KEY }}
      API_FOOTBALL_KEY: ${{ secrets.API_FOOTBALL_KEY }}
      WANDB_DISABLED: "true"

    steps:
      - name: Step 00 - Checkout
        uses: actions/checkout@v4

      - name: Step 01 - Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Step 02 - Install dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install --upgrade pandas numpy requests python-dateutil unidecode rapidfuzz pyarrow scikit-learn pyyaml

      - name: Step 03 - Validate required secrets
        shell: bash
        run: |
          set -euo pipefail
          [ -n "${THEODDS_API_KEY:-}" ] || { echo "::error::Missing THEODDS_API_KEY secret"; exit 1; }
          [ -n "${API_FOOTBALL_KEY:-}" ] || { echo "::error::Missing API_FOOTBALL_KEY secret"; exit 1; }

      - name: Step 04 - Prepare data folders (no stubs)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p data/history data/in data/out data/aliases models
          [ -s "data/aliases/team_aliases.json" ] || echo "{}" > data/aliases/team_aliases.json
          if [ ! -s "${SOURCE_CSV}" ]; then
            echo "::error::Missing ${SOURCE_CSV}. Provide real upcoming matches."
            exit 1
          fi

      - name: Step 05 - Define run variables
        shell: bash
        run: |
          set -euo pipefail
          OUT_DIR="data/out/${GITHUB_RUN_ID}"
          mkdir -p "${OUT_DIR}"
          {
            echo "OUT_DIR=${OUT_DIR}"
            echo "FEATURES_PARQUET=data/history/features.parquet"
            echo "SOURCE_CSV_NORM=${OUT_DIR}/matches_norm.csv"
            echo "AUTO_ALIASES_JSON=data/aliases/auto_aliases.json"
            echo "BIVARIATE_CSV=${OUT_DIR}/bivariate.csv"
            echo "MODEL_PKL=${OUT_DIR}/dynamic_model.pkl"
            echo "STATE_JSON=${OUT_DIR}/state_params.json"
            echo "PREDICTIONS_CSV=${OUT_DIR}/predictions.csv"
            echo "CALIBRATOR_PKL=${OUT_DIR}/calibrator.pkl"
            echo "PREDICTIONS_CALIB_CSV=${OUT_DIR}/predictions_calibrated.csv"
            echo "CONSENSUS_CSV=${OUT_DIR}/odds_consensus.csv"
            echo "KELLY_BETS_CSV=${OUT_DIR}/bets_kelly.csv"
            echo "LOTECA_CARD_CSV=${OUT_DIR}/loteca_card.csv"
          } >> "$GITHUB_ENV"

      - name: Step 06 - Update history
        shell: bash
        run: |
          set -euo pipefail
          [ -f scripts/update_history.py ] || { echo "::error::scripts/update_history.py not found"; exit 1; }
          python -m scripts.update_history --since_days 30 --out "data/history/results.csv"
          [ -s "data/history/results.csv" ] || { echo "::error::results.csv empty"; exit 1; }

      - name: Step 07 - Feature engineering
        shell: bash
        run: |
          set -euo pipefail
          [ -f scripts/feature_engineer.py ] || { echo "::error::scripts/feature_engineer.py not found"; exit 2; }
          python -m scripts.feature_engineer --history "data/history/results.csv" --out "${FEATURES_PARQUET}" --ewma 0.20
          test -s "${FEATURES_PARQUET}" || { echo "::error::features.parquet not generated"; exit 2; }

      - name: Step 08 - Enrich features with API Football
        shell: bash
        run: |
          set -euo pipefail
          [ -f scripts/enrich_api_football.py ] || { echo "::error::scripts/enrich_api_football.py not found"; exit 2; }
          python -m scripts.enrich_api_football --features_in "${FEATURES_PARQUET}" --features_out "${FEATURES_PARQUET}"
          echo "[enrich] done"

      - name: Step 09 - Train dynamic model
        shell: bash
        run: |
          set -euo pipefail
          [ -f scripts/train_dynamic_model.py ] || { echo "::error::scripts/train_dynamic_model.py not found"; exit 2; }
          python -m scripts.train_dynamic_model --features "${FEATURES_PARQUET}" --out_state "${STATE_JSON}" --out_model "${MODEL_PKL}"
          test -s "${STATE_JSON}" || { echo "::error::state_params.json not generated"; exit 2; }
          test -s "${MODEL_PKL}" || { echo "::error::dynamic_model.pkl not generated"; exit 2; }

      - name: Step 10 - Normalize matches
        shell: bash
        run: |
          set -euo pipefail
          [ -f scripts/normalize_matches.py ] || { echo "::error::scripts/normalize_matches.py not found"; exit 3; }
          python -m scripts.normalize_matches --in_csv "${SOURCE_CSV}" --out_csv "${SOURCE_CSV_NORM}"
          test -s "${SOURCE_CSV_NORM}" || { echo "::error::matches_norm.csv not generated"; exit 3; }

      - name: Step 11 - Auto aliases harvest
        shell: bash
        run: |
          set -euo pipefail
          [ -f scripts/harvest_aliases.py ] || { echo "::error::scripts/harvest_aliases.py not found"; exit 3; }
          python -m scripts.harvest_aliases --source_csv "${SOURCE_CSV_NORM}" --lookahead_hours 72 --out_json "${AUTO_ALIASES_JSON}"
          test -s "${AUTO_ALIASES_JSON}" || { echo "::error::auto_aliases.json empty"; exit 3; }

      - name: Step 12 - Preflight TheOddsAPI Serie A
        shell: bash
        run: |
          set -euo pipefail
          printf '%s\n' \
"import os,json,urllib.request,urllib.parse" \
"api=os.environ['THEODDS_API_KEY']" \
"regions=os.environ.get('REGIONS','eu')" \
"qs=urllib.parse.urlencode({'regions':regions,'markets':'h2h','dateFormat':'iso','oddsFormat':'decimal','apiKey':api})" \
"url=f'https://api.the-odds-api.com/v4/sports/soccer_brazil_campeonato/odds?{qs}'" \
"with urllib.request.urlopen(url, timeout=25) as r: data=json.load(r)" \
"print('[preflight] events Serie A =', len(data))" \
          > /tmp/preflight_theodds.py
          python /tmp/preflight_theodds.py

      - name: Step 13 - Ingest TheOddsAPI primary
        shell: bash
        run: |
          set -euo pipefail
          [ -f scripts/ingest_odds_theoddsapi.py ] || { echo "::error::scripts/ingest_odds_theoddsapi.py not found"; exit 5; }
          python -m scripts.ingest_odds_theoddsapi --rodada "${OUT_DIR}" --regions "${REGIONS}" --source_csv "${SOURCE_CSV_NORM}"

      - name: Step 14 - Fallback direct fetch if TheOddsAPI empty
        shell: bash
        run: |
          set -euo pipefail
          OUT="${OUT_DIR}/odds_theoddsapi.csv"
          L=0; [ -s "$OUT" ] && L="$(wc -l < "$OUT" | tr -d ' ')"
          if [ "$L" -le 1 ]; then
            printf '%s\n' \
"import os,csv,json,urllib.request,urllib.parse" \
"API=os.environ['THEODDS_API_KEY']" \
"REG=os.environ.get('REGIONS','eu')" \
"OUT=os.path.join(os.environ['OUT_DIR'],'odds_theoddsapi.csv')" \
"def fetch(sport):" \
"    qs=urllib.parse.urlencode(dict(apiKey=API,regions=REG,markets='h2h',oddsFormat='decimal',dateFormat='iso'))" \
"    url=f'https://api.the-odds-api.com/v4/sports/{sport}/odds?{qs}'" \
"    with urllib.request.urlopen(url, timeout=25) as r: return json.load(r)" \
"rows=[]" \
"for sport in ('soccer_brazil_campeonato','soccer_brazil_serie_b'):" \
"    data=fetch(sport) or []" \
"    for ev in data:" \
"        home=ev.get('home_team') or ''" \
"        away=ev.get('away_team') or ''" \
"        best={'home':None,'draw':None,'away':None}" \
"        for b in ev.get('bookmakers',[]) or []:" \
"            for mk in b.get('markets',[]) or []:" \
"                if mk.get('key')!='h2h': continue" \
"                for o in mk.get('outcomes',[]) or []:" \
"                    n=o.get('name',''); p=o.get('price',None)" \
"                    if p is None: continue" \
"                    if n==home: best['home']=max(best['home'] or 0,p)" \
"                    elif n==away: best['away']=max(best['away'] or 0,p)" \
"                    elif n.lower()=='draw': best['draw']=max(best['draw'] or 0,p)" \
"        if all(best[k] for k in ('home','draw','away')):" \
"            rows.append([home,away,best['home'],best['draw'],best['away']])" \
"with open(OUT,'w',newline='',encoding='utf-8') as f:" \
"    w=csv.writer(f); w.writerow(['team_home','team_away','odds_home','odds_draw','odds_away']); w.writerows(rows)" \
"print('[fallback] wrote',OUT,'rows=',len(rows))" \
            > /tmp/fallback_theodds.py
            python /tmp/fallback_theodds.py
          fi
          [ -s "$OUT" ] && [ "$(wc -l < "$OUT" | tr -d ' ')" -gt 1 ] || { echo "::error::No odds from TheOddsAPI (primary and fallback)"; exit 5; }

      - name: Step 15 - Ingest API Football odds (complementary but required)
        shell: bash
        run: |
          set -euo pipefail
          [ -f scripts/ingest_odds_apifootball.py ] || { echo "::error::scripts/ingest_odds_apifootball.py not found"; exit 5; }
          python -m scripts.ingest_odds_apifootball --rodada "${OUT_DIR}" --source_csv "${SOURCE_CSV_NORM}" || true

      - name: Step 16 - Consensus odds
        shell: bash
        run: |
          set -euo pipefail
          [ -f scripts/consensus_odds_safe.py ] || { echo "::error::scripts/consensus_odds_safe.py not found"; exit 6; }
          python -m scripts.consensus_odds_safe --rodada "${OUT_DIR}" --strict || true
          OUT_FILE="${CONSENSUS_CSV}"
          if [ ! -s "${OUT_FILE}" ]; then
            cp "${OUT_DIR}/odds_theoddsapi.csv" "${OUT_FILE}"
            echo "::notice::Consensus used only TheOddsAPI"
          fi
          header="$(head -n1 "${OUT_FILE}" | tr -d '\r')"
          for c in team_home team_away odds_home odds_draw odds_away; do
            echo "${header}" | grep -qiE "(^|,)$c(,|$)" || { echo "::error::missing column '$c' in odds_consensus.csv"; exit 6; }
          done

      - name: Step 17 - Bivariate estimator
        shell: bash
        run: |
          set -euo pipefail
          [ -f scripts/bivariate_estimator.py ] || { echo "::error::scripts/bivariate_estimator.py not found"; exit 7; }
          python -m scripts.bivariate_estimator --history "${FEATURES_PARQUET}" --matches "${SOURCE_CSV_NORM}" --out "${BIVARIATE_CSV}"
          test -s "${BIVARIATE_CSV}" || { echo "::error::bivariate.csv not generated"; exit 7; }

      - name: Step 18 - Predict with dynamic model
        shell: bash
        run: |
          set -euo pipefail
          [ -f scripts/predict_dynamic_model.py ] || { echo "::error::scripts/predict_dynamic_model.py not found"; exit 8; }
          python -m scripts.predict_dynamic_model --model "${MODEL_PKL}" --state "${STATE_JSON}" --matches "${SOURCE_CSV_NORM}" --out "${PREDICTIONS_CSV}"
          test -s "${PREDICTIONS_CSV}" || { echo "::error::predictions.csv not generated"; exit 8; }

      - name: Step 19 - Train calibrator (or identity if not enough data)
        shell: bash
        run: |
          set -euo pipefail
          if [ "${TRAIN_CALIBRATOR}" = "true" ]; then
            [ -f scripts/calibrator_train.py ] || { echo "::error::scripts/calibrator_train.py not found"; exit 9; }
            if ! python -m scripts.calibrator_train --history "${FEATURES_PARQUET}" --out "${CALIBRATOR_PKL}"; then
              printf '%s\n' \
"import pickle,os" \
"with open(os.environ['CALIBRATOR_PKL'],'wb') as f: pickle.dump({'type':'identity'},f)" \
              > /tmp/cal_id.py
              python /tmp/cal_id.py
            fi
          else
            printf '%s\n' \
"import pickle,os" \
"with open(os.environ['CALIBRATOR_PKL'],'wb') as f: pickle.dump({'type':'identity'},f)" \
            > /tmp/cal_id.py
            python /tmp/cal_id.py
          fi
          test -s "${CALIBRATOR_PKL}" || { echo "::error::calibrator.pkl not generated"; exit 9; }

      - name: Step 20 - Apply calibration
        shell: bash
        run: |
          set -euo pipefail
          printf '%s\n' \
"import csv,os,pickle" \
"pred=os.environ['PREDICTIONS_CSV']; out=os.environ['PREDICTIONS_CALIB_CSV']" \
"with open(os.environ['CALIBRATOR_PKL'],'rb') as f: cal=pickle.load(f)" \
"rows=[]" \
"with open(pred,newline='',encoding='utf-8') as f:" \
"    r=csv.DictReader(f)" \
"    for row in r:" \
"        ph,pd,pa=float(row['p_home']),float(row['p_draw']),float(row['p_away'])" \
"        s=ph+pd+pa" \
"        if s>0: ph,pd,pa=ph/s,pd/s,pa/s" \
"        rows.append([row['team_home'],row['team_away'],ph,pd,pa])" \
"with open(out,'w',newline='',encoding='utf-8') as f:" \
"    w=csv.writer(f); w.writerow(['team_home','team_away','p_home','p_draw','p_away']); w.writerows(rows)" \
"print('[calibration] wrote',out,'rows=',len(rows))" \
          > /tmp/apply_cal.py
          python /tmp/apply_cal.py
          test -s "${PREDICTIONS_CALIB_CSV}" || { echo "::error::predictions_calibrated.csv not generated"; exit 9; }

      - name: Step 21 - Kelly staking 1x2
        shell: bash
        run: |
          set -euo pipefail
          printf '%s\n' \
"import csv,os" \
"BANK=float(os.environ.get('BANKROLL','1000'))" \
"FRAC=float(os.environ.get('KELLY_FRACTION','0.5'))" \
"CAP=float(os.environ.get('KELLY_CAP','0.10'))" \
"ROUND_TO=int(os.environ.get('ROUND_TO','1'))" \
"pred=os.environ['PREDICTIONS_CALIB_CSV']" \
"odds=os.environ['CONSENSUS_CSV']" \
"out=os.environ['KELLY_BETS_CSV']" \
"odict={}" \
"with open(odds,newline='',encoding='utf-8') as f:" \
"    r=csv.DictReader(f)" \
"    for row in r:" \
"        key=(row['team_home'],row['team_away'])" \
"        odict[key]=(float(row['odds_home']),float(row['odds_draw']),float(row['odds_away']))" \
"rows=[]" \
"def kelly(p,o,frac,cap):" \
"    b=o-1.0" \
"    k=(p*b-(1-p))/b if b>0 else 0.0" \
"    k=max(0.0,min(k*frac,cap))" \
"    return k" \
"with open(pred,newline='',encoding='utf-8') as f:" \
"    r=csv.DictReader(f)" \
"    for row in r:" \
"        key=(row['team_home'],row['team_away'])" \
"        if key not in odict: continue" \
"        ph,pd,pa=float(row['p_home']),float(row['p_draw']),float(row['p_away'])" \
"        oh,od,oa=odict[key]" \
"        kh,kd,ka=kelly(ph,oh,FRAC,CAP),kelly(pd,od,FRAC,CAP),kelly(pa,oa,FRAC,CAP)" \
"        side,k,o,p=max([('H',kh,oh,ph),('D',kd,od,pd),('A',ka,oa,pa)],key=lambda x:x[1])" \
"        stake=round(BANK*k,ROUND_TO)" \
"        ev=p*o-1" \
"        rows.append([row['team_home'],row['team_away'],side,p,o,k,stake,ev])" \
"with open(out,'w',newline='',encoding='utf-8') as f:" \
"    w=csv.writer(f)" \
"    w.writerow(['team_home','team_away','side','p','odds','kelly_frac','stake','expected_value'])" \
"    w.writerows(rows)" \
"print('[kelly] bets ->',out,'rows=',len(rows))" \
          > /tmp/kelly.py
          python /tmp/kelly.py
          test -s "${KELLY_BETS_CSV}" || { echo "::error::bets_kelly.csv not generated"; exit 10; }

      - name: Step 22 - Loteca card top 14
        shell: bash
        run: |
          set -euo pipefail
          printf '%s\n' \
"import csv,os" \
"TOP=int(os.environ.get('KELLY_TOP_N','14'))" \
"bets=os.environ['KELLY_BETS_CSV']" \
"out=os.environ['LOTECA_CARD_CSV']" \
"rows=[]" \
"with open(bets,newline='',encoding='utf-8') as f:" \
"    r=csv.DictReader(f); rows=list(r)" \
"rows.sort(key=lambda x:(float(x['stake']),float(x['expected_value'])),reverse=True)" \
"rows=rows[:TOP]" \
"with open(out,'w',newline='',encoding='utf-8') as f:" \
"    w=csv.writer(f)" \
"    w.writerow(['jogo','home','away','palpite','stake','odds','p','ev'])" \
"    for i,row in enumerate(rows,1):" \
"        palpite={'H':'1','D':'X','A':'2'}[row['side']]" \
"        w.writerow([i,row['team_home'],row['team_away'],palpite,row['stake'],row['odds'],row['p'],row['expected_value']])" \
"print('[loteca] card ->',out,'rows=',len(rows))" \
          > /tmp/loteca_card.py
          python /tmp/loteca_card.py
          test -s "${LOTECA_CARD_CSV}" || { echo "::error::loteca_card.csv not generated"; exit 11; }

      - name: Step 23 - Final listing
        shell: bash
        run: |
          set -euo pipefail
          ls -lh "${OUT_DIR}"
          echo "OUT_DIR=${OUT_DIR}"