name: End-to-end Odds + News + ML (Loteca)

on:
  workflow_dispatch:

permissions:
  contents: read

env:
  SEASON: "2025"
  REGIONS: "uk,eu,us,au"
  LOOKAHEAD_DAYS: "3"
  BANKROLL: "1000"
  KELLY_FRACTION: "0.5"
  KELLY_CAP: "0.1"
  KELLY_TOP_N: "14"
  ROUND_TO: "1"
  DEBUG: "true"

jobs:
  end2end:
    runs-on: ubuntu-latest
    env:
      THEODDS_API_KEY: ${{ secrets.THEODDS_API_KEY }}
      X_RAPIDAPI_KEY: ${{ secrets.X_RAPIDAPI_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: ${{ runner.os }}-pip-

      - name: Install dependencies (strict)
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install requests pandas numpy pyyaml unidecode
          fi

      - name: Prepare run folders and env
        id: prep
        run: |
          set -euo pipefail
          if [ -z "${THEODDS_API_KEY:-}" ]; then echo "::error::THEODDS_API_KEY secret not set"; exit 1; fi
          if [ -z "${X_RAPIDAPI_KEY:-}" ]; then echo "::error::X_RAPIDAPI_KEY secret not set"; exit 1; fi

          RUN_ID="$(date +%s)"
          echo "RUN_ID=${RUN_ID}" >> "$GITHUB_OUTPUT"
          echo "OUT_DIR=data/out/${RUN_ID}" >> "$GITHUB_ENV"

          mkdir -p "data/in" "data/out/${RUN_ID}"

          # aliases.json
          if [ ! -s "data/in/aliases.json" ]; then
            echo '{"teams": {}}' > data/in/aliases.json
          fi

          # matches_source.csv obrigatório
          if [ ! -s "data/in/matches_source.csv" ]; then
            echo "::error::Arquivo obrigatório ausente: data/in/matches_source.csv"
            exit 2
          fi

          # DEBUG_FLAG
          if [ "${DEBUG:-}" = "true" ]; then
            echo "DEBUG_FLAG=--debug" >> "$GITHUB_ENV"
          else
            echo "DEBUG_FLAG=" >> "$GITHUB_ENV"
          fi

      - name: Normalize and validate matches_source -> whitelist
        run: |
          set -euo pipefail
          SRC="data/in/matches_source.csv"
          DST_DIR="${OUT_DIR}"
          DST_RAW="${DST_DIR}/matches_whitelist_raw.csv"
          DST_TMP="${DST_DIR}/matches_whitelist_tmp.csv"
          DST_FINAL="${DST_DIR}/matches_whitelist.csv"

          tr -d '\r' < "$SRC" | sed -E 's/[[:space:]]+/ /g; s/ *, */,/g; s/^ //; s/ $//' > "$DST_RAW"

          header="$(head -n1 "$DST_RAW")"
          h_low="$(echo "$header" | tr '[:upper:]' '[:lower:]')"
          for need in match_id home away; do
            echo "$h_low" | grep -qiE "(^|,)$need(,|$)" || { echo "::error::missing column '$need' in matches_source.csv"; exit 2; }
          done

          awk -F',' -v OFS=',' '
            function rmuf(s){ gsub(/\/[A-Za-z]{2}($|[^A-Za-z])/, "", s); return s }
            function deacc(s){
              gsub(/á|à|ã|â|ä/,"a",s); gsub(/Á|À|Ã|Â|Ä/,"A",s);
              gsub(/é|ê|è|ë/,"e",s);  gsub(/É|Ê|È|Ë/,"E",s);
              gsub(/í|î|ì|ï/,"i",s);  gsub(/Í|Î|Ì|Ï/,"I",s);
              gsub(/ó|ô|ò|õ|ö/,"o",s); gsub(/Ó|Ô|Ò|Õ|Ö/,"O",s);
              gsub(/ú|û|ù|ü/,"u",s);  gsub(/Ú|Û|Ù|Ü/,"U",s);
              gsub(/ç/,"c",s);        gsub(/Ç/,"C",s);
              gsub(/ñ/,"n",s);        gsub(/Ñ/,"N",s);
              return s
            }
            function trim(s){ sub(/^ +/,"",s); sub(/ +$/,"",s); return s }
            function map_pt_en(s,  l){
              l=tolower(s)
              if(l=="estonia") return "Estonia"
              if(l=="italia") return "Italy"
              if(l=="bulgaria") return "Bulgaria"
              if(l=="turquia") return "Turkey"
              if(l=="espanha") return "Spain"
              if(l=="georgia") return "Georgia"
              if(l=="servia" || l=="serbia/ser" || l=="serbia") return "Serbia"
              if(l=="albania") return "Albania"
              if(l=="portugal") return "Portugal"
              if(l=="irlanda") return "Ireland"
              if(l=="holanda" || l=="paises baixos") return "Netherlands"
              if(l=="finlandia") return "Finland"
              if(l=="romenia") return "Romania"
              if(l=="austria") return "Austria"
              if(l=="dinamarca") return "Denmark"
              if(l=="grecia") return "Greece"
              if(l=="lituania") return "Lithuania"
              if(l=="polonia") return "Poland"
              if(l=="ponte preta") return "Ponte Preta"
              if(l=="guarani") return "Guarani"
              if(l=="palmeiras") return "Palmeiras"
              if(l=="juventude") return "Juventude"
              if(l=="criciuma") return "Criciuma"
              if(l=="america/mg" || l=="america mg" || l=="america-mg" || l=="america mineiro") return "America Mineiro"
              if(l=="cuiaba") return "Cuiaba"
              if(l=="coritiba") return "Coritiba"
              if(l=="novorizontino" || l=="gremio novorizontino" || l=="grêmio novorizontino") return "Gremio Novorizontino"
              if(l=="operario" || l=="operario pr" || l=="operario/pr" || l=="operário/pr") return "Operario PR"
              return s
            }
            NR==1{
              for(i=1;i<=NF;i++){
                k=tolower($i);
                if(k=="match_id") mi=i;
                if(k=="home") hi=i;
                if(k=="away") ai=i;
              }
              if(!(mi&&hi&&ai)){ print "::error::header mapping failed"; exit 2 }
              print "match_id","home","away";
              next
            }
            {
              mid=$mi; home=$hi; away=$ai;
              home=rmuf(home); away=rmuf(away);
              home=deacc(home); away=deacc(away);
              home=trim(home); away=trim(away);
              home=map_pt_en(home); away=map_pt_en(away);
              if(mid=="" || home=="" || away==""){ print "::error::empty field on line " NR; exit 2 }
              if(seen[mid]++){ print "::error::duplicate match_id: " mid; exit 2 }
              print mid,home,away
            }
          ' "$DST_RAW" > "$DST_TMP"

          LINES=$(wc -l < "$DST_TMP" | tr -d " ")
          [ "$LINES" -ge 2 ] || { echo "::error::whitelist empty after normalization"; exit 3; }

          mv "$DST_TMP" "$DST_FINAL"

          echo "===== Preview normalized whitelist ====="
          head -n 30 "$DST_FINAL" || true

      - name: Validate team names before ingest (pre-flight)
        run: |
          set -euo pipefail
          WL="${OUT_DIR}/matches_whitelist.csv"
          echo "=== Unique teams from whitelist ==="
          awk -F',' 'NR>1{print $2; print $3}' "$WL" | sort -u | nl -ba

      - name: Ingest THEODDSAPI (mandatory)
        run: |
          set -euo pipefail
          [ -f scripts/ingest_odds_theoddsapi.py ] || { echo "::error::scripts/ingest_odds_theoddsapi.py not found"; exit 5; }

          a=0; until [ $a -ge 3 ]; do
            python -m scripts.ingest_odds_theoddsapi \
              --rodada "${OUT_DIR}" \
              --regions "${REGIONS}" \
              --aliases "data/in/aliases.json" \
              ${DEBUG_FLAG} && break
            a=$((a+1)); echo "retry theodds: $a/3"; sleep $((5*a))
          done

          OUT="${OUT_DIR}/odds_theoddsapi.csv"
          test -s "$OUT" || { echo "::error::odds_theoddsapi.csv not generated"; exit 5; }

          header="$(head -n1 "$OUT" | tr -d '\r')"
          for c in match_id home away odds_home odds_draw odds_away; do
            echo "$header" | grep -qiE "(^|,)$c(,|$)" || { echo "::error::missing column '$c' in odds_theoddsapi.csv"; exit 5; }
          done

      - name: Ingest API-Football (RapidAPI, mandatory)
        run: |
          set -euo pipefail
          [ -f scripts/ingest_odds_apifootball_rapidapi.py ] || { echo "::error::scripts/ingest_odds_apifootball_rapidapi.py not found"; exit 5; }

          a=0; until [ $a -ge 3 ]; do
            python -m scripts.ingest_odds_apifootball_rapidapi \
              --rodada "${OUT_DIR}" \
              --season "${SEASON}" \
              ${DEBUG_FLAG} && break
            a=$((a+1)); echo "retry apifootball: $a/3"; sleep $((5*a))
          done

          OUT="${OUT_DIR}/odds_apifootball.csv"
          test -s "$OUT" || { echo "::error::odds_apifootball.csv not generated"; exit 5; }

          header="$(head -n1 "$OUT" | tr -d '\r')"
          for c in match_id home away odds_home odds_draw odds_away; do
            echo "$header" | grep -qiE "(^|,)$c(,|$)" || { echo "::error::missing column '$c' in odds_apifootball.csv"; exit 5; }
          done

      - name: Consensus odds (STRICT)
        run: |
          set -euo pipefail
          [ -f scripts/consensus_odds_safe.py ] || { echo "::error::scripts/consensus_odds_safe.py not found"; exit 6; }
          python -m scripts.consensus_odds_safe --rodada "${OUT_DIR}" --strict

          OUT_FILE="${OUT_DIR}/odds_consensus.csv"
          test -s "$OUT_FILE" || { echo "::error::odds_consensus.csv not generated"; exit 6; }
          header="$(head -n1 "$OUT_FILE" | tr -d '\r')"
          for c in team_home team_away odds_home odds_draw odds_away; do
            echo "$header" | grep -qiE "(^|,)$c(,|$)" || { echo "::error::missing column '$c' in odds_consensus.csv"; exit 6; }
          done

          echo "=== Preview odds_consensus.csv ==="
          head -n 20 "$OUT_FILE" || true

      - name: Calibrate probabilities from consensus
        run: |
          set -euo pipefail
          python - <<'PY'
import pandas as pd, numpy as np
from unidecode import unidecode
from pathlib import Path

out_dir = Path("${OUT_DIR}")
wl = pd.read_csv(out_dir/"matches_whitelist.csv")
cons = pd.read_csv(out_dir/"odds_consensus.csv")

def norm(s):
    return unidecode(str(s).strip().lower())

wl["home_n"] = wl["home"].map(norm)
wl["away_n"] = wl["away"].map(norm)
cons["home_n"] = cons["team_home"].map(norm)
cons["away_n"] = cons["team_away"].map(norm)

m = wl.merge(cons, on=["home_n","away_n"], how="inner",
             suffixes=("",""))
if m.empty:
    raise SystemExit("::error::no matches joined between whitelist and consensus. Check name normalization.")

# implied probs and margin normalization
for col in ("odds_home","odds_draw","odds_away"):
    if (m[col] <= 1).any():
        raise SystemExit(f"::error::invalid odds <=1 found in consensus: column {col}")

inv = pd.DataFrame({
    "ih": 1.0/m["odds_home"].astype(float),
    "ix": 1.0/m["odds_draw"].astype(float),
    "ia": 1.0/m["odds_away"].astype(float),
})
s = inv.sum(axis=1).replace(0, np.nan)
m["p_home"] = inv["ih"]/s
m["p_draw"] = inv["ix"]/s
m["p_away"] = inv["ia"]/s

cols = ["match_id","home","away","team_home","team_away","odds_home","odds_draw","odds_away","p_home","p_draw","p_away"]
m[cols].to_csv(out_dir/"calibrated_probs.csv", index=False)
print("calibrated_probs.csv written with", len(m), "rows")
PY
          test -s "${OUT_DIR}/calibrated_probs.csv" || { echo "::error::calibrated_probs.csv not generated"; exit 7; }

      - name: XG - Univariate (feature engineering)
        run: |
          set -euo pipefail
          python - <<'PY'
import pandas as pd
from pathlib import Path
out_dir = Path("${OUT_DIR}")
df = pd.read_csv(out_dir/"calibrated_probs.csv")

# Proxy xG simples a partir das probabilidades:
df["xg_home"] = 1.8*df["p_home"] + 0.9*df["p_draw"]
df["xg_away"] = 1.8*df["p_away"] + 0.9*df["p_draw"]

cols = ["match_id","team_home","team_away","p_home","p_draw","p_away","xg_home","xg_away"]
df[cols].to_csv(out_dir/"xg_univariate.csv", index=False)
print("xg_univariate.csv written with", len(df))
PY
          test -s "${OUT_DIR}/xg_univariate.csv" || { echo "::error::xg_univariate.csv not generated"; exit 8; }

      - name: XG - Bivariate (feature engineering)
        run: |
          set -euo pipefail
          python - <<'PY'
import pandas as pd
from pathlib import Path
out_dir = Path("${OUT_DIR}")
u = pd.read_csv(out_dir/"xg_univariate.csv")

# Interações simples
u["xg_sum"] = u["xg_home"] + u["xg_away"]
u["xg_diff"] = u["xg_home"] - u["xg_away"]
u["p_max"] = u[["p_home","p_draw","p_away"]].max(axis=1)

cols = ["match_id","team_home","team_away","xg_home","xg_away","xg_sum","xg_diff","p_max"]
u[cols].to_csv(out_dir/"xg_bivariate.csv", index=False)
print("xg_bivariate.csv written with", len(u))
PY
          test -s "${OUT_DIR}/xg_bivariate.csv" || { echo "::error::xg_bivariate.csv not generated"; exit 9; }

      - name: Kelly stakes
        run: |
          set -euo pipefail
          [ -f scripts/publish_kelly.py ] || { echo "::error::scripts/publish_kelly.py não encontrado"; exit 25; }
          python scripts/publish_kelly.py --rodada "${OUT_DIR}" ${DEBUG_FLAG}
          test -s "${OUT_DIR}/kelly_stakes.csv" || { echo "::error::kelly_stakes.csv não gerado"; exit 25; }

      - name: Montar Cartão Loteca
        run: |
          set -euo pipefail
          python - <<'PY'
import pandas as pd
from pathlib import Path
from unidecode import unidecode

out_dir = Path("${OUT_DIR}")
wl  = pd.read_csv(out_dir/"matches_whitelist.csv")
cal = pd.read_csv(out_dir/"calibrated_probs.csv")

def norm(s): return unidecode(str(s).strip().lower())
wl["home_n"] = wl["home"].map(norm); wl["away_n"] = wl["away"].map(norm)
cal["home_n"] = cal["team_home"].map(norm); cal["away_n"] = cal["team_away"].map(norm)
m = wl.merge(cal, on=["home_n","away_n"], how="left")

def pick(row):
    vals = {"1":row.get("p_home",0), "X":row.get("p_draw",0), "2":row.get("p_away",0)}
    return max(vals, key=vals.get)

m["palpite"] = m.apply(pick, axis=1)
m["prob_max"] = m[["p_home","p_draw","p_away"]].max(axis=1)

lines = []
for i, r in m.iterrows():
    idx = i+1
    h, a = r["home"], r["away"]
    tip = r["palpite"]
    prb = r["prob_max"]
    lines.append(f"{idx:02d} - {h} x {a}  -> {tip}  ({prb:.1%})")

card = "\n".join(lines)
(out_dir/"loteca_card.txt").write_text(card, encoding="utf-8")
print(card)
PY
          test -s "${OUT_DIR}/loteca_card.txt" || { echo "::error::loteca_card.txt não gerado"; exit 30; }
          echo "==== Preview loteca_card.txt ===="
          head -n 50 "${OUT_DIR}/loteca_card.txt" || true

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: loteca_${{ steps.prep.outputs.RUN_ID }}
          path: |
            ${{ env.OUT_DIR }}/matches_whitelist.csv
            ${{ env.OUT_DIR }}/odds_theoddsapi.csv
            ${{ env.OUT_DIR }}/odds_apifootball.csv
            ${{ env.OUT_DIR }}/odds_consensus.csv
            ${{ env.OUT_DIR }}/calibrated_probs.csv
            ${{ env.OUT_DIR }}/xg_univariate.csv
            ${{ env.OUT_DIR }}/xg_bivariate.csv
            ${{ env.OUT_DIR }}/kelly_stakes.csv
            ${{ env.OUT_DIR }}/loteca_card.txt